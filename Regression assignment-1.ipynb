{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac46aff-eec0-452f-8b44-f5d226208c29",
   "metadata": {},
   "source": [
    "Simple linear regression involves predicting a dependent variable using only one independent variable. It fits a straight line to the data. For example, predicting a student's test score based on the number of hours they studied.\n",
    "\n",
    "Multiple linear regression, on the other hand, involves predicting a dependent variable using two or more independent variables. It fits a plane (or hyperplane in higher dimensions) to the data. For example, predicting a house's price based on its size, number of bedrooms, and distance from the city center."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947587b2-52fe-48d3-91ae-7790d8a1ff1e",
   "metadata": {},
   "source": [
    "The assumptions of linear regression are:\n",
    "\n",
    "1. Linearity: The relationship between the independent variables and the dependent variable should be linear.\n",
    "2. Independence: The residuals (the differences between observed and predicted values) should be independent of each other.\n",
    "3. Homoscedasticity: The variance of the residuals should be constant across all levels of the independent variables.\n",
    "4. Normality: The residuals should follow a normal distribution.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, you can:\n",
    "\n",
    "1. Visual Inspection: Plot the data and residuals to visually assess linearity, independence, and homoscedasticity.\n",
    "2. Residual Analysis: Plot residuals against predicted values to check for homoscedasticity. A scatter plot of residuals against each independent variable can help identify potential patterns or non-linearity.\n",
    "3. Normality Tests: Perform statistical tests like the Shapiro-Wilk test or visually inspect a histogram or a QQ plot of the residuals to assess normality.\n",
    "4. Multicollinearity: Check for multicollinearity among independent variables using techniques like variance inflation factor (VIF) or correlation matrix.\n",
    "\n",
    "If assumptions are violated, appropriate remedial actions such as transformations, using robust regression techniques, or removing outliers may be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a510a3-734a-41a4-ac5d-6c8b850d2130",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope represents the change in the dependent variable for a one-unit change in the independent variable, while the intercept represents the value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "For example, consider a linear regression model predicting a person's weight (dependent variable) based on their height (independent variable). If the slope coefficient is 0.5, it means that for every one-inch increase in height, the person's weight is expected to increase by 0.5 pounds. If the intercept is 100 pounds, it means that a person with a height of zero inches would be expected to weigh 100 pounds, which is not meaningful in this context but illustrates the concept.\n",
    "\n",
    "So, in summary:\n",
    "- Slope: Represents the rate of change of the dependent variable with respect to the independent variable.\n",
    "- Intercept: Represents the value of the dependent variable when the independent variable is zero, which may or may not be meaningful depending on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6312fa21-028d-4fab-a125-21fa35b6a2c4",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm used to minimize the cost function or error function in machine learning models. It's based on the idea of iteratively moving towards the minimum of a function by taking steps proportional to the negative of the gradient (slope) of the function at the current point.\n",
    "\n",
    "Here's how it works:\n",
    "1. Initialize the parameters (weights) of the model randomly.\n",
    "2. Compute the gradient of the cost function with respect to these parameters.\n",
    "3. Update the parameters in the opposite direction of the gradient to minimize the cost function.\n",
    "4. Repeat steps 2 and 3 until convergence (when the change in cost function is negligible) or for a predefined number of iterations.\n",
    "\n",
    "Gradient Descent comes in different variations like Batch Gradient Descent, Stochastic Gradient Descent, and Mini-batch Gradient Descent, each with its own advantages and disadvantages in terms of computational efficiency and convergence speed.\n",
    "\n",
    "In machine learning, Gradient Descent is used in various algorithms like linear regression, logistic regression, neural networks, and support vector machines to iteratively update the model parameters and minimize the error or cost function, thus improving the model's performance. It's a fundamental optimization technique essential for training complex machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b2bb2-4357-4ab3-ba46-c33091fddc6b",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that allows for the prediction of a dependent variable based on two or more independent variables. In multiple linear regression, the relationship between the dependent variable and the independent variables is modeled as a linear combination of the independent variables, each weighted by a coefficient, plus an intercept term.\n",
    "\n",
    "Mathematically, the multiple linear regression model can be represented as:\n",
    "\n",
    "\\[Y = \\beta_0 + \\beta_1*X_1 + \\beta_2*X_2 + ... + \\beta_n*X_n + \\varepsilon\\]\n",
    "\n",
    "Where:\n",
    "- \\(Y\\) is the dependent variable.\n",
    "- \\(\\beta_0\\) is the intercept term.\n",
    "- \\(\\beta_1, \\beta_2, ..., \\beta_n\\) are the coefficients of the independent variables \\(X_1, X_2, ..., X_n\\) respectively.\n",
    "- \\(X_1, X_2, ..., X_n\\) are the independent variables.\n",
    "- \\(\\varepsilon\\) is the error term.\n",
    "\n",
    "The main difference between multiple linear regression and simple linear regression is the number of independent variables used to predict the dependent variable. In simple linear regression, only one independent variable is used, while in multiple linear regression, two or more independent variables are used. This allows for a more complex and nuanced modeling of the relationship between the dependent variable and the independent variables, potentially leading to more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9676af-43f6-4f1e-adaa-d1c15ab34f75",
   "metadata": {},
   "source": [
    "Multicollinearity occurs in multiple linear regression when two or more independent variables are highly correlated with each other. This can cause issues such as unstable coefficient estimates, inflated standard errors, and difficulty in interpreting the individual effects of the correlated variables on the dependent variable.\n",
    "\n",
    "To detect multicollinearity:\n",
    "1. Correlation Matrix: Calculate the correlation coefficients between pairs of independent variables. High correlation coefficients (close to 1 or -1) indicate multicollinearity.\n",
    "2. Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF measures how much the variance of an estimated regression coefficient increases if your predictors are correlated. VIF values greater than 10 or 5 are often considered indicative of multicollinearity.\n",
    "\n",
    "To address multicollinearity:\n",
    "1. Remove Redundant Variables: If two or more independent variables are highly correlated, consider removing one of them from the model.\n",
    "2. Combine Variables: If possible, combine highly correlated variables into a single composite variable.\n",
    "3. Regularization Techniques: Use regularization techniques like Ridge Regression or Lasso Regression, which penalize large coefficients and can help mitigate the effects of multicollinearity.\n",
    "4. Principal Component Analysis (PCA): Transform the original correlated variables into a smaller set of uncorrelated variables using PCA.\n",
    "5. Collect More Data: Sometimes multicollinearity can be caused by insufficient data. Collecting more data may help mitigate this issue by providing a more diverse range of observations.\n",
    "\n",
    "By detecting and addressing multicollinearity, you can improve the stability and interpretability of the multiple linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600bff0-a8bf-4398-add4-91157d472c6a",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as an nth degree polynomial function. Unlike linear regression, which fits a straight line to the data, polynomial regression can capture non-linear relationships between the variables.\n",
    "\n",
    "Mathematically, the polynomial regression model can be represented as:\n",
    "\n",
    "\\[Y = \\beta_0 + \\beta_1*X + \\beta_2*X^2 + ... + \\beta_n*X^n + \\varepsilon\\]\n",
    "\n",
    "Where:\n",
    "- \\(Y\\) is the dependent variable.\n",
    "- \\(X\\) is the independent variable.\n",
    "- \\(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n\\) are the coefficients of the polynomial terms.\n",
    "- \\(n\\) is the degree of the polynomial.\n",
    "- \\(\\varepsilon\\) is the error term.\n",
    "\n",
    "The main difference between polynomial regression and linear regression is the functional form of the model. While linear regression assumes a linear relationship between the independent and dependent variables, polynomial regression allows for more complex, non-linear relationships by fitting a polynomial curve to the data.\n",
    "\n",
    "In summary, polynomial regression extends linear regression by allowing for non-linear relationships between variables through the use of polynomial terms, providing a more flexible and versatile modeling approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249c8bc-32a8-4021-b274-0dd17ee08411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
